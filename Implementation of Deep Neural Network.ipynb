{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation of Deep Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the text and normalization-one hot representation\n",
    "\n",
    "X = np.loadtxt(\"./FeaturesX.csv\")\n",
    "Y = np.loadtxt(\"./LabelsY.csv\")\n",
    "X = X.reshape((800,7))\n",
    "Y = Y.astype(int)\n",
    "Y = pd.DataFrame(Y, columns=['labels'])\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import OneHotEncoder \n",
    "\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "Features = min_max_scaler.fit_transform(X)\n",
    "\n",
    "Y_onehot = pd.get_dummies(Y, columns=['labels'])\n",
    "Target = Y_onehot.values\n",
    "\n",
    "# adding bias\n",
    "unityMatrix = np.ones((np.shape(Features)[0],1))    # bias\n",
    "final_x = np.concatenate((unityMatrix, Features), axis=1) # adding bias to feature vector\n",
    "\n",
    "# train-test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, X_test, y_train, y_test = train_test_split(final_x, Target, test_size=0.3, random_state=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration  0 Mean Squared Error:  0.5964651999022419\n",
      "Iteration  100 Mean Squared Error:  0.409405141244384\n",
      "Iteration  200 Mean Squared Error:  0.3813169260748589\n",
      "Iteration  300 Mean Squared Error:  0.22959214802828587\n",
      "Iteration  400 Mean Squared Error:  0.22659255874844741\n",
      "Iteration  500 Mean Squared Error:  0.223969206616718\n",
      "Iteration  600 Mean Squared Error:  0.22165318430513137\n",
      "Iteration  700 Mean Squared Error:  0.21959893900992766\n",
      "Iteration  800 Mean Squared Error:  0.21776531982360167\n",
      "Iteration  900 Mean Squared Error:  0.21611795892148924\n",
      "Iteration  1000 Mean Squared Error:  0.21462991184592875\n",
      "Iteration  1100 Mean Squared Error:  0.21328118312086566\n",
      "Iteration  1200 Mean Squared Error:  0.21205759137000235\n",
      "Iteration  1300 Mean Squared Error:  0.21094930720425922\n",
      "Iteration  1400 Mean Squared Error:  0.2099493277359253\n",
      "Iteration  1500 Mean Squared Error:  0.2090521036433669\n",
      "Iteration  1600 Mean Squared Error:  0.20825247444248318\n",
      "Iteration  1700 Mean Squared Error:  0.20754498411693045\n",
      "Iteration  1800 Mean Squared Error:  0.2069235616693595\n",
      "Iteration  1900 Mean Squared Error:  0.20638148832870162\n",
      "\n",
      "Accuracy Score from the Test Dataset:  0.6513888888888889\n"
     ]
    }
   ],
   "source": [
    "# model specifications\n",
    "size_train = 560\n",
    "Ni=8; Nh=64; No=3;\n",
    "\n",
    "# parameter and array initialization\n",
    "learningRate = 0.01\n",
    "Ntrials = 2000\n",
    "wh=np.random.randn(Nh,Ni); dwh=np.zeros(wh.shape) \n",
    "wo=np.random.randn(No,Nh); dwo=np.zeros(wo.shape) \n",
    "error=np.array([])\n",
    "\n",
    "# Training\n",
    "for trials in range(Ntrials):     \n",
    "\n",
    "  h=1/(1+np.exp(-wh@x_train.T ))        # hidden activation for all pattern\n",
    "  y=1/(1+np.exp(-wo@h))                 # output for all pattern\n",
    "\n",
    "  do=y*(1-y)*(y_train.T-y)/size_train   # delta output\n",
    "  dh=h*(1-h)*(wo.transpose()@do)        # delta backpropagated  \n",
    "\n",
    "\n",
    "  # update weights with momentum\n",
    "  dwo=0.9*dwo+do@h.T\n",
    "  wo=wo+learningRate*dwo\n",
    "  dwh=0.9*dwh+dh@x_train\n",
    "  wh=wh+learningRate*dwh\n",
    "\n",
    "  error=np.append(error,np.mean((y_train.T-y)**2))\n",
    "  \n",
    "  if trials % 100 == 0:\n",
    "    print('Iteration ',trials, 'Mean Squared Error: ',np.mean((y_train.T-y)**2))\n",
    "\n",
    "\n",
    "# Testing\n",
    "h=1/(1+np.exp(-wh@X_test.T)) \n",
    "predicted=1/(1+np.exp(-wo@h))        \n",
    "predicted = np.round(predicted)\n",
    "\n",
    "# Accuracy\n",
    "accuracyScore = (y_test == predicted.T)\n",
    "print('\\nAccuracy Score from the Test Dataset: ', accuracyScore.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Error\")\n",
    "plt.plot(error)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price (grands)</th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>sqft_living</th>\n",
       "      <th>sqft_lot</th>\n",
       "      <th>floors</th>\n",
       "      <th>waterfront</th>\n",
       "      <th>view</th>\n",
       "      <th>condition</th>\n",
       "      <th>grade</th>\n",
       "      <th>sqft_above</th>\n",
       "      <th>sqft_basement</th>\n",
       "      <th>age</th>\n",
       "      <th>renovated_age</th>\n",
       "      <th>sqft_living15</th>\n",
       "      <th>sqft_lot15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>221.9</td>\n",
       "      <td>3</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1180</td>\n",
       "      <td>5650</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>1180</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0</td>\n",
       "      <td>1340</td>\n",
       "      <td>5650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>538.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.25</td>\n",
       "      <td>2570</td>\n",
       "      <td>7242</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>2170</td>\n",
       "      <td>400</td>\n",
       "      <td>66</td>\n",
       "      <td>40</td>\n",
       "      <td>1690</td>\n",
       "      <td>7639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>180.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.00</td>\n",
       "      <td>770</td>\n",
       "      <td>10000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>770</td>\n",
       "      <td>0</td>\n",
       "      <td>84</td>\n",
       "      <td>0</td>\n",
       "      <td>2720</td>\n",
       "      <td>8062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>604.0</td>\n",
       "      <td>4</td>\n",
       "      <td>3.00</td>\n",
       "      <td>1960</td>\n",
       "      <td>5000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>1050</td>\n",
       "      <td>910</td>\n",
       "      <td>52</td>\n",
       "      <td>0</td>\n",
       "      <td>1360</td>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>510.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1680</td>\n",
       "      <td>8080</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>1680</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>1800</td>\n",
       "      <td>7503</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   price (grands)  bedrooms  bathrooms  sqft_living  sqft_lot  floors  \\\n",
       "0           221.9         3       1.00         1180      5650     1.0   \n",
       "1           538.0         3       2.25         2570      7242     2.0   \n",
       "2           180.0         2       1.00          770     10000     1.0   \n",
       "3           604.0         4       3.00         1960      5000     1.0   \n",
       "4           510.0         3       2.00         1680      8080     1.0   \n",
       "\n",
       "   waterfront  view  condition  grade  sqft_above  sqft_basement  age  \\\n",
       "0           0     0          3      7        1180              0   62   \n",
       "1           0     0          3      7        2170            400   66   \n",
       "2           0     0          3      6         770              0   84   \n",
       "3           0     0          5      7        1050            910   52   \n",
       "4           0     0          3      8        1680              0   30   \n",
       "\n",
       "   renovated_age  sqft_living15  sqft_lot15  \n",
       "0              0           1340        5650  \n",
       "1             40           1690        7639  \n",
       "2              0           2720        8062  \n",
       "3              0           1360        5000  \n",
       "4              0           1800        7503  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load csv file\n",
    "df = pd.read_csv('./houses1.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price (grands)</th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>sqft_living</th>\n",
       "      <th>sqft_lot</th>\n",
       "      <th>floors</th>\n",
       "      <th>waterfront</th>\n",
       "      <th>view</th>\n",
       "      <th>condition</th>\n",
       "      <th>grade</th>\n",
       "      <th>sqft_above</th>\n",
       "      <th>sqft_basement</th>\n",
       "      <th>age</th>\n",
       "      <th>renovated_age</th>\n",
       "      <th>sqft_living15</th>\n",
       "      <th>sqft_lot15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>21613.000000</td>\n",
       "      <td>21613.000000</td>\n",
       "      <td>21613.000000</td>\n",
       "      <td>21613.000000</td>\n",
       "      <td>2.161300e+04</td>\n",
       "      <td>21613.000000</td>\n",
       "      <td>21613.000000</td>\n",
       "      <td>21613.000000</td>\n",
       "      <td>21613.000000</td>\n",
       "      <td>21613.000000</td>\n",
       "      <td>21613.000000</td>\n",
       "      <td>21613.000000</td>\n",
       "      <td>21613.000000</td>\n",
       "      <td>21613.000000</td>\n",
       "      <td>21613.000000</td>\n",
       "      <td>21613.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>540.088142</td>\n",
       "      <td>3.370842</td>\n",
       "      <td>2.114757</td>\n",
       "      <td>2079.899736</td>\n",
       "      <td>1.510697e+04</td>\n",
       "      <td>1.494309</td>\n",
       "      <td>0.007542</td>\n",
       "      <td>0.234303</td>\n",
       "      <td>3.409430</td>\n",
       "      <td>7.656873</td>\n",
       "      <td>1788.390691</td>\n",
       "      <td>291.509045</td>\n",
       "      <td>45.994864</td>\n",
       "      <td>2.380882</td>\n",
       "      <td>1986.552492</td>\n",
       "      <td>12768.455652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>367.127196</td>\n",
       "      <td>0.930062</td>\n",
       "      <td>0.770163</td>\n",
       "      <td>918.440897</td>\n",
       "      <td>4.142051e+04</td>\n",
       "      <td>0.539989</td>\n",
       "      <td>0.086517</td>\n",
       "      <td>0.766318</td>\n",
       "      <td>0.650743</td>\n",
       "      <td>1.175459</td>\n",
       "      <td>828.090978</td>\n",
       "      <td>442.575043</td>\n",
       "      <td>29.373411</td>\n",
       "      <td>12.359528</td>\n",
       "      <td>685.391304</td>\n",
       "      <td>27304.179631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>75.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>290.000000</td>\n",
       "      <td>5.200000e+02</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>290.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>399.000000</td>\n",
       "      <td>651.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>321.950000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.750000</td>\n",
       "      <td>1427.000000</td>\n",
       "      <td>5.040000e+03</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>1190.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1490.000000</td>\n",
       "      <td>5100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>450.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.250000</td>\n",
       "      <td>1910.000000</td>\n",
       "      <td>7.618000e+03</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>1560.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>42.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1840.000000</td>\n",
       "      <td>7620.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>645.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>2550.000000</td>\n",
       "      <td>1.068800e+04</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>2210.000000</td>\n",
       "      <td>560.000000</td>\n",
       "      <td>66.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2360.000000</td>\n",
       "      <td>10083.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>7700.000000</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>13540.000000</td>\n",
       "      <td>1.651359e+06</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>9410.000000</td>\n",
       "      <td>4820.000000</td>\n",
       "      <td>117.000000</td>\n",
       "      <td>114.000000</td>\n",
       "      <td>6210.000000</td>\n",
       "      <td>871200.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       price (grands)      bedrooms     bathrooms   sqft_living      sqft_lot  \\\n",
       "count    21613.000000  21613.000000  21613.000000  21613.000000  2.161300e+04   \n",
       "mean       540.088142      3.370842      2.114757   2079.899736  1.510697e+04   \n",
       "std        367.127196      0.930062      0.770163    918.440897  4.142051e+04   \n",
       "min         75.000000      0.000000      0.000000    290.000000  5.200000e+02   \n",
       "25%        321.950000      3.000000      1.750000   1427.000000  5.040000e+03   \n",
       "50%        450.000000      3.000000      2.250000   1910.000000  7.618000e+03   \n",
       "75%        645.000000      4.000000      2.500000   2550.000000  1.068800e+04   \n",
       "max       7700.000000     33.000000      8.000000  13540.000000  1.651359e+06   \n",
       "\n",
       "             floors    waterfront          view     condition         grade  \\\n",
       "count  21613.000000  21613.000000  21613.000000  21613.000000  21613.000000   \n",
       "mean       1.494309      0.007542      0.234303      3.409430      7.656873   \n",
       "std        0.539989      0.086517      0.766318      0.650743      1.175459   \n",
       "min        1.000000      0.000000      0.000000      1.000000      1.000000   \n",
       "25%        1.000000      0.000000      0.000000      3.000000      7.000000   \n",
       "50%        1.500000      0.000000      0.000000      3.000000      7.000000   \n",
       "75%        2.000000      0.000000      0.000000      4.000000      8.000000   \n",
       "max        3.500000      1.000000      4.000000      5.000000     13.000000   \n",
       "\n",
       "         sqft_above  sqft_basement           age  renovated_age  \\\n",
       "count  21613.000000   21613.000000  21613.000000   21613.000000   \n",
       "mean    1788.390691     291.509045     45.994864       2.380882   \n",
       "std      828.090978     442.575043     29.373411      12.359528   \n",
       "min      290.000000       0.000000      2.000000       0.000000   \n",
       "25%     1190.000000       0.000000     20.000000       0.000000   \n",
       "50%     1560.000000       0.000000     42.000000       0.000000   \n",
       "75%     2210.000000     560.000000     66.000000       0.000000   \n",
       "max     9410.000000    4820.000000    117.000000     114.000000   \n",
       "\n",
       "       sqft_living15     sqft_lot15  \n",
       "count   21613.000000   21613.000000  \n",
       "mean     1986.552492   12768.455652  \n",
       "std       685.391304   27304.179631  \n",
       "min       399.000000     651.000000  \n",
       "25%      1490.000000    5100.000000  \n",
       "50%      1840.000000    7620.000000  \n",
       "75%      2360.000000   10083.000000  \n",
       "max      6210.000000  871200.000000  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()  # various static summary of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalization of dataset\n",
    "df = (df - df.mean())/df.std()  \n",
    "\n",
    "# split the data into features and target\n",
    "X=df.iloc[:,1:].values\n",
    "Y=df.iloc[:,0].values\n",
    "X_1=df.iloc[:,3].values\n",
    "\n",
    "# reshape the target matrix\n",
    "Y = Y.reshape((21613,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Linear Regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spli the data into train and test set\n",
    "X1_train = df.iloc[1:-1:2,3].values\n",
    "X1_test = df.iloc[0:-1:2,3].values\n",
    "Y_train = df.iloc[1:-1:2,0].values\n",
    "Y_test = df.iloc[0:-1:2,0].values\n",
    "X_train = df.iloc[1:-1:2,1:].values\n",
    "X_test = df.iloc[0:-1:2,1:].values\n",
    "\n",
    "# reshape the target matrix\n",
    "Y_train = Y_train.reshape((10806,1))\n",
    "Y_test = Y_test.reshape((10806,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression for feature vector X_1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear regression function for feature X_1\n",
    "def linear_regression(X, Y, lr, repetition):\n",
    "    m = 0 # params\n",
    "    c = 0 \n",
    "    loss_X1 = np.array([]) \n",
    "    n = float(len(X))  # number of instances in train set\n",
    "    for i in range(repetition): \n",
    "        Y_pred = X * m + c  \n",
    "        loss_X1 = np.append(loss_X1,sum((Y_pred-Y)**2))  # loss per interation \n",
    "        #derM = (-2/n) * sum(X * (Y - Y_pred))  # derivation od cost function\n",
    "        #derC = (-2/n) * sum(Y - Y_pred)  \n",
    "        m = m - (lr * ((-2/n) * sum(X * (Y - Y_pred))))  # update the parameter m\n",
    "        c = c - (lr * ((-2/n) * sum(Y - Y_pred)))  # update the parameter c\n",
    "        \n",
    "    return m,c,loss_X1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for prediction\n",
    "def prediction(X, Y, m, c):\n",
    "    n = float(len(X_1))\n",
    "    Y_pred = X * m + c\n",
    "    mse = (1/n) * sum((Y_pred-Y)**2)\n",
    "    return mse    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m,c,loss_X1 = linear_regression(X1_train,Y_train,0.0001,10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = prediction(X1_test, Y_test, m, c)\n",
    "print(\"MSE for X_1 feature\",mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning Curve for feature matrix X_1\n",
    "plt.loglog(loss)\n",
    "plt.xlabel('Iterations'); plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Linear Regression for feature matrix X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multivariant Linear Regression for feature matrix X\n",
    "def multilinear_regression(X, Y, lr, repetition):\n",
    "    m = np.ones((1,len(X[0])))\n",
    "    c = 0\n",
    "    n = float(len(X)) \n",
    "    loss=np.array([])\n",
    "    for i in range(repetition): \n",
    "        Y_pred = X @ m.T + c  \n",
    "        loss=np.append(loss,sum((Y_pred-Y)**2))\n",
    "        D_m = -2/n * np.sum((Y - Y_pred)*X ,axis=0)  \n",
    "        D_c =-2/n * sum(Y - Y_pred)  \n",
    "        m = m - lr * D_m  \n",
    "        c = c - lr * D_c  \n",
    "    return m,c,loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction function\n",
    "def prediction(X, Y, m, c):\n",
    "    n = float(len(X_1))\n",
    "    Y_pred = X @ m.T + c\n",
    "    mse = (1/n) * sum((Y_pred-Y)**2)\n",
    "    return mse    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params, c, loss_X = multilinear_regression(X_train, Y_train, 0.0001, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = prediction(X_test, Y_test, params, c)\n",
    "print(\"MSE for X features\",mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning Curve for feature matrix X\n",
    "plt.loglog(loss_X)\n",
    "plt.xlabel('Iterations'); plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 4.3 Plot the model\n",
    "\n",
    "#### Linear model for feature X_1(Price vs Area)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making predictions\n",
    "plt.scatter(X_1, Y,s=5,color='red') \n",
    "plt.plot(X_1,Y_pred)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Linear Model for feature X(actual values vs predicated values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(Y_pred, Y,s=5,color='red')\n",
    "plt.xlabel('Predicted Values'); plt.ylabel('Actual Values')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 4.4 Ridge Regression\n",
    "\n",
    "#### Ridge Linear Regression for X_1 feature vector "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear regression function for feature X_1\n",
    "def linear_regression_Ridge(X, Y, lr, repetition, alpha):\n",
    "    m = 0 # params\n",
    "    c = 0 \n",
    "    loss_X1 = np.array([]) \n",
    "    n = float(len(X))  # number of instances in train set\n",
    "    for i in range(repetition): \n",
    "        Y_pred = m*X + c  # linear regression function\n",
    "        loss_X1 = np.append(loss_X1,sum((Y_pred-Y)**2))  # loss per interation \n",
    "        derM = (-2/n) * sum(X * (Y - Y_pred))  # derivation od cost function\n",
    "        derC = (-2/n) * sum(Y - Y_pred)  \n",
    "        m = m*(1 - ((lr*alpha)/n)) - lr * derM\n",
    "        c = c - (lr * derC)  # update the parameter c\n",
    "        \n",
    "    return m,c,loss_X1 \n",
    "\n",
    "# function for prediction\n",
    "def prediction(X, Y, m, c):\n",
    "    n = float(len(X_1))\n",
    "    Y_pred = X * m + c\n",
    "    mse = (1/n) * sum((Y_pred-Y)**2)\n",
    "    return mse    \n",
    "\n",
    "params_ridge,c_ridge,loss_X1 = linear_regression_Ridge(X1_train,Y_train,0.0001,10000,0.1)\n",
    "\n",
    "mse = prediction(X_test, Y_test, params, c)\n",
    "print(\"MSE for X features\",mse)\n",
    "\n",
    "# learning Curve for feature matrix X\n",
    "plt.loglog(loss_X)\n",
    "plt.xlabel('Iterations'); plt.ylabel('Loss')\n",
    "plt.show()\n",
    "\n",
    "#### Ridge Regression for X matrix\n",
    "\n",
    "# Building the model\n",
    "def multilinear_regression_Ridge(X, Y, lr, re, alpha):\n",
    "    m = np.ones((1,len(X[0])))\n",
    "    #m_identity = np.identity((len(X[0])))\n",
    "    c = 0\n",
    "    n = float(len(X)) # Number of elements in X\n",
    "    mse=np.array([])\n",
    "    # Performing Gradient Descent \n",
    "    for i in range(re): \n",
    "        Y_pred = X @ m.T + c  # The current predicted value of Y\n",
    "        mse = np.append(mse,sum((Y_pred-Y)**2))\n",
    "        D_m = -2/n * np.sum((Y - Y_pred)*X ,axis=0)  # Derivative wrt m\n",
    "        c = c - lr*(-2/n * sum(Y - Y_pred))  # Update c\n",
    "        m = m*(1 - ((lr*alpha)/n)) - lr * D_m\n",
    "    return m,c,mse\n",
    "\n",
    "# prediction function\n",
    "def prediction(X, Y, m, c):\n",
    "    n = float(len(X_1))\n",
    "    Y_pred = X @ m.T + c\n",
    "    mse = (1/n) * sum((Y_pred-Y)**2)\n",
    "    return mse    \n",
    "\n",
    "m,c,mse = multilinear_regression_Ridge(X_train,Y_train,0.0001, 10000, 0.1)\n",
    "\n",
    "##### MSE for Feature X_1 is 0.25508278\n",
    "\n",
    "\n",
    "MSE_ridge = prediction(X_test, Y_test, m, c)\n",
    "print(\"MSE for X features\",MSE_ridge)\n",
    "\n",
    "# learning graph\n",
    "plt.loglog(mse)\n",
    "plt.xlabel('Iterations'); plt.ylabel('Loss')\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(Y_pred, Y,s=5,color='red')\n",
    "plt.xlabel('Predicted Values'); plt.ylabel('Actual Values')\n",
    "plt.show()\n",
    "\n",
    "mse_reg=np.array([])\n",
    "list_penalty = [0.1, 0.01, 1, 10, 100, 5, 50, 500]\n",
    "for i in list_penalty:\n",
    "    m,c,mse = linear_regression_Ridge(X,Y,0.0001, 10000,i)\n",
    "    Y_pred = X @ m.T + c\n",
    "    mse_reg = np.append(mse,sum((Y_pred-Y)**2))\n",
    "\n",
    "# learning graph\n",
    "plt.loglog(mse_reg)\n",
    "plt.xlabel('Iterations'); plt.ylabel('Loss')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 4.5 Ridge Regression with Momentum\n",
    "\n",
    "# Building the model\n",
    "def linear_regression_Ridge_momentum(X, Y, lr, re, alpha, beta):\n",
    "    m = np.ones((1,len(X[0])))\n",
    "    m_identity = np.identity((len(X[0])))\n",
    "    c = 0\n",
    "    n = float(len(X)) # Number of elements in X\n",
    "    mse=np.array([])\n",
    "    V_dc = 0\n",
    "    V_dm = np.zeros((1,len(X[0])))\n",
    "    # Performing Gradient Descent \n",
    "    for i in range(re): \n",
    "        Y_pred = X @ m.T + c  # The current predicted value of Y\n",
    "        mse = np.append(mse,sum((Y_pred-Y)**2))\n",
    "        D_m = -2/n * np.sum((Y - Y_pred)*X ,axis=0)  # Derivative wrt m\n",
    "        D_c = -2/n * sum(Y - Y_pred)\n",
    "        V_dm = (beta * V_dm) + (1- beta)*D_m\n",
    "        V_dc = (beta * V_dc) + (1- beta)*D_c\n",
    "        c = c - lr* V_dc  # Update c\n",
    "        m = m*(1 - ((lr*alpha)/n)) - lr * V_dm\n",
    "    return m,c,mse\n",
    "\n",
    "m,c,mse = linear_regression_Ridge_momentum(X,Y,0.0001, 10000, 0.1, 0.9)\n",
    "\n",
    "Y_pred = X @ m.T + c\n",
    "\n",
    "# learning graph\n",
    "plt.loglog(mse)\n",
    "plt.xlabel('Iterations'); plt.ylabel('Loss')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 4.6 Optimized Polynomial Regression\n",
    "\n",
    "# Building the model\n",
    "def polynomial_regression_optimized (X, Y, lr, re, alpha, beta):\n",
    "    m = np.zeros((1,len(X[0])))\n",
    "    c = 0\n",
    "    n = float(len(X)) # Number of elements in X\n",
    "    mse=np.array([])\n",
    "    V_dc = 0\n",
    "    V_dm = np.zeros((1,len(X[0])))\n",
    "    # Performing Gradient Descent \n",
    "    for i in range(re): \n",
    "        Y_pred = X @ m.T + c  # The current predicted value of Y\n",
    "        mse = np.append(mse,sum((Y_pred-Y)**2))\n",
    "        D_m = -2/n * np.sum((Y - Y_pred)*X ,axis=0)  # Derivative wrt m\n",
    "        D_c = -2/n * sum(Y - Y_pred)\n",
    "        V_dm = (beta * V_dm) + (1- beta)*D_m\n",
    "        V_dc = (beta * V_dc) + (1- beta)*D_c\n",
    "        c = c - lr* V_dc  # Update c\n",
    "        m = m*(1 - ((lr*alpha)/n)) - lr * V_dm\n",
    "    return m,c,mse\n",
    "\n",
    "m\n",
    "\n",
    "X_1a = X_1.reshape((21613,1))\n",
    "X_1a.shape\n",
    "\n",
    "X_sq = np.array(X_1a**2)\n",
    "X_sq = X_sq.reshape((21613,1))\n",
    "X_mod = np.concatenate((X_1a,X_sq ), axis=1)\n",
    "X_mod.shape\n",
    "\n",
    "m,c,mse = polynomial_regression_optimized(X_mod,Y,0.0001, 10000, 0.1,0.9)\n",
    "\n",
    "Y_pred = Y_pred.ravel()\n",
    "Y_pred.shape\n",
    "\n",
    "plt.plot(X_1, Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Reference:\n",
    "https://www.geeksforgeeks.org/find-average-list-python/\n",
    "https://machinelearningmastery.com/k-fold-cross-validation/\n",
    "irisdata program given on the brightspace\n",
    "mlp program given on the brightspace\n",
    "https://www.youtube.com/watch?v=k8fTYJPd3_I\n",
    "https://www.geeksforgeeks.org/python-pandas-dataframe/#targetText=Pandas%20DataFrame%20is%20two%2Ddimensional,fashion%20in%20rows%20and%20columns."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
